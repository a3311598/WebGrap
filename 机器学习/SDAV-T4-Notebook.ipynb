{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://www.cems.uwe.ac.uk/~pa-legg/uwecyber/images/uwe.png\" width=300>\n",
    "<img src=\"https://www.cems.uwe.ac.uk/~pa-legg/uwecyber/images/uwecyber_acecse_200.jpg\" width=300>\n",
    "\n",
    "# UFCFEL-15-3 Security Data Analytics and Visualisation\n",
    "# Portfolio Task 4: Fake News Classification (2021)\n",
    "---\n",
    "\n",
    "The completion of this worksheet is worth **30%** towards your portfolio for the UFCFEL-15-3 Security Data Analytics and Visualisation (SDAV) module.\n",
    "\n",
    "### Task\n",
    "***\n",
    "\n",
    "In this task, you will need to develop a system that can perform text analytics to classify news articles as either being fake news or true. You should look to use the ***ski-kit learn*** Python library as part of your work - you will find it useful to conduct research and to examine the user guide: [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html). \n",
    "\n",
    "The provided dataset has two csv files: one contains 23481 fake news articles, the other contains 21417 true news articles. The data consists of the following columns: title (contains news headlines), text (contains news content/article), subject (type of news), and date (date the news was published).\n",
    "\n",
    "You will need to consider how to pre-process the data so that it is suitable for further analysis and for use with a machine learning classifier. This will include how the data is structured, how the output class is denoted, any cleansing of the text that may be required (e.g., removal of stopwords, stemming, n-grams).\n",
    "\n",
    "You should then show how you can deploy 3 different ML classifiers on the data, using the scikit-learn library to achieve this. As an example, you may choose the following 3 algorithms (or you may explore suitable alternatives):\n",
    "\n",
    "- logreg_cv = LogisticRegression(random_state=0)\n",
    "- dt_cv=DecisionTreeClassifier()\n",
    "- nb_cv=MultinomialNB(alpha=0.1) \n",
    "\n",
    "You should make use of visualisation to illustrate the distinguishing characteristics of the dataset classes, including the news categories and the top 20 n-grams of the data in each respective class.\n",
    "\n",
    "You will need to describe the analytical process you have taken using Markdown, and you will need to report your overall final accuracy for the classifier.\n",
    "\n",
    "### Assessment and Marking\n",
    "***\n",
    "\n",
    "| Criteria | 0-39 | 40-49 | 50-59 | 60-69 | 70-84 | 85-100 |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| **Suitable use of text pre-processing (30%)** | No evidence of progress | A limited attempt to address this criteria | A working solution but perhaps not optimal | Good approach to the problem | Very good approach to the problem | Excellent approach to the problem |\n",
    "| **Use of 3 Machine Learning classifiers and reporting their performance (30%)**  | No evidence of progress | A limited attempt to address this criteria |  A working solution but perhaps not optimal | Good approach to the problem | Very good approach to the problem | Excellent approach to the problem |\n",
    "| **Visualisation techniques to understand the pre-processing and classification stages (20%)**  | No evidence of progress | A limited attempt to address this criteria |  A working solution but perhaps not optimal | Good approach to the problem | Very good approach to the problem | Excellent approach to the problem |\n",
    "| **Clarity and professional presentation (20%)**  | No evidence of progress | A limited attempt to address this criteria | Some evidence of markdown commentary | Good approach to the problem | Very good approach to the problem | Excellent approach to the problem |\n",
    "\n",
    "You will need to implement your final solution in the Notebook format, with Markdown annotation -  you should use this notebook file as a template for your submission. You are also expected to complete the assignment self-assessment.\n",
    "\n",
    "Your submission should include:\n",
    "- HTML export of your complete assignment in notebook format.\n",
    "- Original ipynb source file of your notebook.\n",
    "\n",
    "### Self-Assessment\n",
    "---\n",
    "\n",
    "For each criteria, please reflect on the marking rubric and indicate what grade you would expect to receive for the work that you are submitting. For your own personal development and learning, it is important to reflect on your work and to attempt to assess this careful. Do think carefully about both positive aspects of your work, as well as any limitations you may have faced.\n",
    "\n",
    "- **Suitable use of text pre-processing (30%)**: You estimate that your grade will be 75.\n",
    "\n",
    "- **Use of 3 Machine Learning classifiers and reporting their performance (30%)**: You estimate that your grade will be 85.\n",
    "\n",
    "- **Visualisation techniques to understand the pre-processing and classification stages (20%)**: You estimate that your grade will be 65.\n",
    "\n",
    "- **Clarity and professional presentation (20%)**: You estimate that your grade will be 65.\n",
    "\n",
    "Please provide a minimum of two sentences to comment and reflect on your own self-assessment: This task is very chanlenging for me. I will find a more suitable way to finish the work.\n",
    "\n",
    "\n",
    "### Contact\n",
    "---\n",
    "\n",
    "Questions about this assignment should be directed to your module leader (Phil.Legg@uwe.ac.uk). You can use the Blackboard Q&A feature to ask questions related to this module and this assignment, as well as the on-site teaching sessions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pandas numpy matplotlib seaborn wordcloud sklearn\n",
    "#! pip install nltk\n",
    "\n",
    "#Basic libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "#Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Miscellanous libraries\n",
    "from collections import Counter\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#reading the fake and true datasets\n",
    "fake_news = pd.read_csv('./T4_data/Fake.csv')\n",
    "true_news = pd.read_csv('./T4_data/True.csv')\n",
    "\n",
    "# print (\"Fake news: \", fake_news.shape)\n",
    "# print (\"True news: \", true_news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## fake_news.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## true_news.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The above code will load in the two datasets, showing the overall size of the datasets and also the first 10 rows from each dataset.\n",
    "\n",
    "Now it is over to you..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fake_news.insert(loc=4, column='state', value=0)\n",
    "fake_news = fake_news.dropna()\n",
    "fake_news = fake_news[~fake_news['title'].isin(['http','https'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "true_news.insert(loc=4, column='state', value=1)\n",
    "true_news = true_news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.concat([fake_news,true_news])\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import machine learning algorithm related Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk import pos_tag, word_tokenize,sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from itertools import chain\n",
    "# from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def cut_word(text):\n",
    "#     sens = sent_tokenize(text)\n",
    "#     cw = [word_tokenize(sentence) for sentence in sens]\n",
    "#     return cw\n",
    "# data['title'] = data['title'].apply(word_tokenize)\n",
    "# data['text'] = data['text'].apply(word_tokenize)\n",
    "# data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "N-gram generation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ngram_list(n, word_list, stop_word_list=None):\n",
    "        \"\"\"\n",
    "        Generate ngrams with width n excluding those that are entirely formed of stop words\n",
    "\n",
    "        Args:\n",
    "            n (int): i.e. 1, 2, 3...\n",
    "            word_list (list of str): list of words\n",
    "            stop_word_list (list of str, Optional): list of words that should be excluded while obtaining\n",
    "                                                    list of ngrams\n",
    "\n",
    "        Returns:\n",
    "            list of str: List of ngrams formed from the given word list except for those that have all their tokes in\n",
    "                         stop words list\n",
    "        \"\"\"\n",
    "        stop_word_set = set(stop_word_list) if stop_word_list else []\n",
    "        all_ngrams = ngrams(word_list, n)\n",
    "        ngram_list = []\n",
    "        for ngram in all_ngrams:\n",
    "            lowered_ngram_tokens = map(lambda token: token.lower(), ngram)\n",
    "            if any(token not in stop_word_set for token in lowered_ngram_tokens):\n",
    "                ngram_list.append(' '.join(ngram))\n",
    "        return ngram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([],columns=[\"Category\",\"Thigram\"])\n",
    "df[\"Category\"] = data[\"subject\"]\n",
    "df[\"Thigram\"] = data[\"text\"].apply(lambda x:ngram_list(3, word_tokenize(x))[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"The volume of N-Gram for each Category\")\n",
    "sns_plot = sns.countplot(x=\"Category\", data=df, order=df['Category'].value_counts().index)\n",
    "sns_plot.figure.set_size_inches(12,8)\n",
    "plt.ylabel(\"the volume of Category\")\n",
    "plt.xlabel(\"Categorys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "# import numpy as np\n",
    "list_title = true_news['title'].values\n",
    "txt =  \" \".join(np.ravel(list_title))\n",
    "w = WordCloud(width = 1000,height = 700,background_color = \"white\",max_words = 30)\n",
    "w.generate(txt)       #加载文本\n",
    "w.to_file(\"wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Determine training data and objectives\n",
    "X = data.iloc[:, 0:4]\n",
    "Y = data.iloc[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split training set and test set\n",
    "x_train,y_train,x_test,y_test= train_test_split(X,Y,test_size=.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#calculate F-IDF\n",
    "vectorizer = TfidfVectorizer(binary = False, decode_error = 'ignore',stop_words = 'english')\n",
    "X_train_counts_tf = vectorizer.fit_transform(x_train[\"text\"].values.tolist())\n",
    "# print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Algorithm evaluation benchmark\n",
    "'''The accuracy of the algorithm is compared by 5-fold cross validation'''\n",
    "num_folds = 5\n",
    "seed = 0\n",
    "scoring = 'accuracy'\n",
    "#评估算法\n",
    "models = {}\n",
    "models['LR'] = LogisticRegression()\n",
    "models['CART'] = DecisionTreeClassifier()\n",
    "models['MNB'] = MultinomialNB()\n",
    "results = []\n",
    "\n",
    "for key in models:\n",
    "    kfold = KFold(n_splits=num_folds,random_state=seed,shuffle=True)\n",
    "    cv_results = cross_val_score(models[key], X_train_counts_tf,x_test, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "#     print('%s:%f(%f)' %(key,cv_results.mean(),cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Parameter tuning\n",
    "param_grid = {}\n",
    "param_grid['C'] = [0.1,5,13,15]\n",
    "model = LogisticRegression()\n",
    "kfold = KFold(n_splits=num_folds,random_state=seed,shuffle=True)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(X=X_train_counts_tf, y=x_test)\n",
    "# print('best : %s param %s' % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Integrated algorithm\n",
    "ensembles = {}\n",
    "ensembles['RF'] = RandomForestClassifier()\n",
    "ensembles['AB'] = AdaBoostClassifier()\n",
    "results = []\n",
    "for key in  ensembles:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n",
    "    cv_results = cross_val_score(ensembles[key], X_train_counts_tf, x_test, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "#     print('%s : %f (%f)' % (key, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Integrated algorithm tuning\n",
    "# param_grid = {}\n",
    "# param_grid['n_estimators'] = [10,100,150,200]\n",
    "# model = RandomForestClassifier()\n",
    "# kfold = KFold(n_splits=num_folds,random_state=seed,shuffle=True)\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "# grid_result = grid.fit(X=X_train_counts_tf, y=x_test)\n",
    "# print('best : %s param %s' % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate model\n",
    "# model = LogisticRegression(C=13)\n",
    "# model.fit(X_train_counts_tf,x_test)\n",
    "# predictions = model.predict(y_test)\n",
    "# print(accuracy_score(y_test, predictions))\n",
    "# print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}